services:
  llama:
    image: ghcr.io/ggml-org/llama.cpp:server
    container_name: llama_cpp
    env_file:
      - .env
    command: >
      -m /models/${LLM_MODEL}
      --port 8000
      --host 0.0.0.0
      --n-gpu-layers 0
      --ctx-size 2048
    volumes:
      - ./models:/models
    ports:
      - "8000:8000"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  db:
    image: postgres:16
    container_name: app_db
    env_file:
      - .env
    environment:
      POSTGRES_DB: ${DB_NAME}
      POSTGRES_USER: ${DB_USER}
      POSTGRES_PASSWORD: ${DB_PASSWORD}
    volumes:
      - pgdata:/var/lib/postgresql/data
    ports:
      - "${DB_PORT}:5432"
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER} -d ${DB_NAME}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  redis:
    image: redis:7
    container_name: app_redis
    env_file:
      - .env
    ports:
      - "${REDIS_PORT}:6379"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3
      start_period: 5s

  seed:
    build: .
    container_name: db_seed
    env_file:
      - .env
    depends_on:
      db:
        condition: service_healthy
    environment:
      DB_PATH: ${DB_PATH}
    command: python seed.py
    restart: "no"

  app:
    build: .
    container_name: app_combinario
    env_file:
      - .env
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
      llama:
        condition: service_healthy
    environment:
      DATABASE_URL: ${DB_URL}
      REDIS_URL: "redis://${REDIS_HOST}:${REDIS_PORT}/${REDIS_DB}"
      LLAMA_BASE_URL: "${LLM_BASE_URL}"
      OPEN_AI_API_KEY: "${OPEN_AI_API_KEY}"
    ports:
      - "8080:8080"
    command: uvicorn app:app --host 0.0.0.0 --port 8080 --reload
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  worker:
    build: .
    container_name: arq_worker
    env_file:
      - .env
    depends_on:
      redis:
        condition: service_healthy
      db:
        condition: service_healthy
      llama:
        condition: service_healthy
    environment:
      DATABASE_URL: ${DB_URL}
      REDIS_URL: "redis://${REDIS_HOST}:${REDIS_PORT}/${REDIS_DB}"
      LLAMA_BASE_URL: "${LLM_BASE_URL}"
      OPEN_AI_API_KEY: "${OPEN_AI_API_KEY}"
    command: arq worker.WorkerSettings
    restart: unless-stopped

volumes:
  pgdata: